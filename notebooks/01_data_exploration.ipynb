{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86106fb2",
   "metadata": {},
   "source": [
    "# Data Exploration - Stress Level Prediction\n",
    "\n",
    "This notebook performs comprehensive data exploration for the stress level prediction project.\n",
    "\n",
    "## Objectives:\n",
    "1. Load and examine the dataset structure\n",
    "2. Perform basic statistical analysis\n",
    "3. Check data quality and missing values\n",
    "4. Analyze correlations between features\n",
    "5. Visualize data distributions and patterns\n",
    "6. Generate automated profiling report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eff9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up paths\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from utils.config import *\n",
    "from data.data_loader import DataLoader\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c862f2a4",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dc115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "data_loader = DataLoader()\n",
    "\n",
    "# Load your dataset - Replace 'your_dataset.csv' with your actual filename\n",
    "# df = data_loader.load_csv('your_dataset.csv')\n",
    "\n",
    "# For demonstration, we'll create a sample dataset\n",
    "# Remove this section when you have your actual data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Create sample stress-related features\n",
    "sample_data = {\n",
    "    'heart_rate': np.random.normal(75, 15, n_samples),\n",
    "    'sleep_hours': np.random.normal(7, 1.5, n_samples),\n",
    "    'exercise_minutes': np.random.exponential(30, n_samples),\n",
    "    'caffeine_intake': np.random.poisson(2, n_samples),\n",
    "    'work_hours': np.random.normal(8, 2, n_samples),\n",
    "    'age': np.random.randint(18, 65, n_samples),\n",
    "    'bmi': np.random.normal(25, 4, n_samples),\n",
    "    'blood_pressure_sys': np.random.normal(120, 20, n_samples),\n",
    "    'blood_pressure_dia': np.random.normal(80, 10, n_samples),\n",
    "    'stress_level': np.random.choice(['Low', 'Medium', 'High'], n_samples, p=[0.3, 0.5, 0.2])\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(sample_data)\n",
    "\n",
    "# Introduce some missing values for demonstration\n",
    "missing_indices = np.random.choice(df.index, size=int(0.05 * len(df)), replace=False)\n",
    "df.loc[missing_indices, 'sleep_hours'] = np.nan\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cd9f15",
   "metadata": {},
   "source": [
    "## 2. Basic Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c52fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic dataset information\n",
    "data_info = data_loader.get_data_info(df)\n",
    "\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Shape: {data_info['shape']}\")\n",
    "print(f\"Columns: {data_info['columns']}\")\n",
    "print(f\"Memory usage: {data_info['memory_usage'] / 1024:.2f} KB\")\n",
    "print(\"\\nData types:\")\n",
    "for col, dtype in data_info['dtypes'].items():\n",
    "    print(f\"  {col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12874e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics\n",
    "print(\"Descriptive Statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfbbe11",
   "metadata": {},
   "source": [
    "## 3. Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2127d5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percentage = (missing_data / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Missing Percentage': missing_percentage\n",
    "})\n",
    "\n",
    "print(\"Missing Values Analysis:\")\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "# Visualize missing values\n",
    "if missing_data.sum() > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    missing_data[missing_data > 0].plot(kind='bar')\n",
    "    plt.title('Missing Values by Column')\n",
    "    plt.xlabel('Columns')\n",
    "    plt.ylabel('Count of Missing Values')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No missing values found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34deedd6",
   "metadata": {},
   "source": [
    "## 4. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2216f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target variable distribution\n",
    "target_col = 'stress_level'  # Update with your actual target column\n",
    "\n",
    "if target_col in df.columns:\n",
    "    print(f\"Target Variable: {target_col}\")\n",
    "    print(\"\\nValue Counts:\")\n",
    "    print(df[target_col].value_counts())\n",
    "    \n",
    "    print(\"\\nProportions:\")\n",
    "    print(df[target_col].value_counts(normalize=True))\n",
    "    \n",
    "    # Visualize target distribution\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Count plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.countplot(data=df, x=target_col)\n",
    "    plt.title('Distribution of Stress Levels')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Pie chart\n",
    "    plt.subplot(1, 2, 2)\n",
    "    df[target_col].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
    "    plt.title('Stress Level Proportions')\n",
    "    plt.ylabel('')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Target column '{target_col}' not found in dataset\")\n",
    "    print(f\"Available columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52665677",
   "metadata": {},
   "source": [
    "## 5. Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c094ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of numerical features\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "if len(numerical_cols) > 0:\n",
    "    n_cols = 3\n",
    "    n_rows = (len(numerical_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    plt.figure(figsize=(15, 4 * n_rows))\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols, 1):\n",
    "        plt.subplot(n_rows, n_cols, i)\n",
    "        sns.histplot(df[col], kde=True)\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel(col)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No numerical columns found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94234d65",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0b684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix for numerical features\n",
    "if len(numerical_cols) > 1:\n",
    "    correlation_matrix = df[numerical_cols].corr()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, linewidths=0.5)\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find highly correlated feature pairs\n",
    "    high_corr_pairs = []\n",
    "    threshold = 0.7\n",
    "    \n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "                high_corr_pairs.append((\n",
    "                    correlation_matrix.columns[i],\n",
    "                    correlation_matrix.columns[j],\n",
    "                    correlation_matrix.iloc[i, j]\n",
    "                ))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print(f\"\\nHighly correlated feature pairs (|correlation| > {threshold}):\")\n",
    "        for pair in high_corr_pairs:\n",
    "            print(f\"  {pair[0]} - {pair[1]}: {pair[2]:.3f}\")\n",
    "    else:\n",
    "        print(f\"\\nNo highly correlated feature pairs found (threshold: {threshold})\")\n",
    "else:\n",
    "    print(\"Not enough numerical features for correlation analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eab236",
   "metadata": {},
   "source": [
    "## 7. Feature vs Target Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0929ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze relationship between features and target variable\n",
    "if target_col in df.columns and len(numerical_cols) > 0:\n",
    "    # Box plots for numerical features vs target\n",
    "    n_cols = 3\n",
    "    n_rows = (len(numerical_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    plt.figure(figsize=(15, 4 * n_rows))\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols, 1):\n",
    "        plt.subplot(n_rows, n_cols, i)\n",
    "        sns.boxplot(data=df, x=target_col, y=col)\n",
    "        plt.title(f'{col} by {target_col}')\n",
    "        plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88b348c",
   "metadata": {},
   "source": [
    "## 8. Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe6be9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers using IQR method\n",
    "def detect_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "print(\"Outlier Analysis:\")\n",
    "for col in numerical_cols:\n",
    "    outliers = detect_outliers(df, col)\n",
    "    outlier_percentage = (len(outliers) / len(df)) * 100\n",
    "    print(f\"{col}: {len(outliers)} outliers ({outlier_percentage:.2f}%)\")\n",
    "\n",
    "# Visualize outliers with box plots\n",
    "if len(numerical_cols) > 0:\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    df[numerical_cols].boxplot()\n",
    "    plt.title('Box Plots for Outlier Detection')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4813fb91",
   "metadata": {},
   "source": [
    "## 9. Data Quality Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90320f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data quality summary\n",
    "quality_summary = {\n",
    "    'Total Rows': len(df),\n",
    "    'Total Columns': len(df.columns),\n",
    "    'Numerical Columns': len(numerical_cols),\n",
    "    'Categorical Columns': len(df.select_dtypes(exclude=[np.number]).columns),\n",
    "    'Missing Values': df.isnull().sum().sum(),\n",
    "    'Duplicate Rows': df.duplicated().sum(),\n",
    "    'Memory Usage (KB)': df.memory_usage(deep=True).sum() / 1024\n",
    "}\n",
    "\n",
    "print(\"Data Quality Summary:\")\n",
    "print(\"=\" * 30)\n",
    "for key, value in quality_summary.items():\n",
    "    if key == 'Memory Usage (KB)':\n",
    "        print(f\"{key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Data quality recommendations\n",
    "print(\"\\nRecommendations:\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "if quality_summary['Missing Values'] > 0:\n",
    "    missing_percentage = (quality_summary['Missing Values'] / (len(df) * len(df.columns))) * 100\n",
    "    print(f\"- Handle {quality_summary['Missing Values']} missing values ({missing_percentage:.2f}% of total data)\")\n",
    "\n",
    "if quality_summary['Duplicate Rows'] > 0:\n",
    "    print(f\"- Remove {quality_summary['Duplicate Rows']} duplicate rows\")\n",
    "\n",
    "if len(high_corr_pairs) > 0:\n",
    "    print(f\"- Consider removing highly correlated features ({len(high_corr_pairs)} pairs found)\")\n",
    "\n",
    "print(\"- Consider feature scaling for numerical variables\")\n",
    "print(\"- Encode categorical variables for machine learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505781b1",
   "metadata": {},
   "source": [
    "## 10. Automated Data Profiling (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c57858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate automated profiling report using ydata-profiling\n",
    "# Uncomment the following lines if you want to generate a detailed report\n",
    "\n",
    "# try:\n",
    "#     from ydata_profiling import ProfileReport\n",
    "#     \n",
    "#     # Generate profile report\n",
    "#     profile = ProfileReport(df, title=\"Stress Level Prediction - Data Profile\", explorative=True)\n",
    "#     \n",
    "#     # Save report\n",
    "#     profile.to_file(\"../reports/data_profile_report.html\")\n",
    "#     print(\"Data profiling report saved to ../reports/data_profile_report.html\")\n",
    "#     \n",
    "# except ImportError:\n",
    "#     print(\"ydata-profiling not installed. Install with: pip install ydata-profiling\")\n",
    "\n",
    "print(\"Data exploration completed!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Clean the data based on findings\")\n",
    "print(\"2. Handle missing values and outliers\")\n",
    "print(\"3. Proceed to feature selection and engineering\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
