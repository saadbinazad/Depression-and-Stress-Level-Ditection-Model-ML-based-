{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f55392f2",
   "metadata": {},
   "source": [
    "# Model Development - Stress Level Prediction\n",
    "\n",
    "This notebook develops and compares different machine learning models for stress level prediction.\n",
    "\n",
    "## Objectives:\n",
    "1. Load processed dataset with selected features\n",
    "2. Split data into training and testing sets\n",
    "3. Train multiple classification models\n",
    "4. Compare model performances\n",
    "5. Perform hyperparameter tuning\n",
    "6. Select the best model\n",
    "7. Save the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26888af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from utils.config import *\n",
    "from models.model_trainer import ModelTrainer\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a92c49e",
   "metadata": {},
   "source": [
    "## 1. Load Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36249e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with selected features\n",
    "try:\n",
    "    final_dataset_path = PROCESSED_DATA_DIR / \"final_dataset.csv\"\n",
    "    df = pd.read_csv(final_dataset_path)\n",
    "    print(f\"Loaded final dataset from: {final_dataset_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Final dataset not found. Please run the feature selection notebook first.\")\n",
    "    # Create sample data for demonstration\n",
    "    np.random.seed(42)\n",
    "    n_samples = 800\n",
    "    \n",
    "    # Create sample features (selected features from previous step)\n",
    "    sample_data = {\n",
    "        'heart_rate': np.random.normal(0, 1, n_samples),\n",
    "        'work_hours': np.random.normal(0, 1, n_samples),\n",
    "        'sleep_hours': np.random.normal(0, 1, n_samples),\n",
    "        'exercise_minutes': np.random.normal(0, 1, n_samples),\n",
    "        'bmi': np.random.normal(0, 1, n_samples),\n",
    "        'caffeine_intake': np.random.normal(0, 1, n_samples),\n",
    "        'stress_level': np.random.choice([0, 1, 2], n_samples, p=[0.3, 0.5, 0.2])\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(sample_data)\n",
    "    # Make features more predictive\n",
    "    df.loc[df['stress_level'] == 2, 'heart_rate'] += 1.5\n",
    "    df.loc[df['stress_level'] == 2, 'work_hours'] += 1.2\n",
    "    df.loc[df['stress_level'] == 0, 'sleep_hours'] += 1.0\n",
    "    df.loc[df['stress_level'] == 0, 'exercise_minutes'] += 0.8\n",
    "    \n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caf5293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "target_col = 'stress_level'\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeature columns: {list(X.columns)}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "target_counts = y.value_counts().sort_index()\n",
    "print(target_counts)\n",
    "print(f\"\\nTarget proportions:\")\n",
    "print(y.value_counts(normalize=True).sort_index())\n",
    "\n",
    "# Visualize target distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "target_counts.plot(kind='bar')\n",
    "plt.title('Target Distribution (Counts)')\n",
    "plt.xlabel('Stress Level')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "target_counts.plot(kind='pie', autopct='%1.1f%%')\n",
    "plt.title('Target Distribution (Proportions)')\n",
    "plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd43a499",
   "metadata": {},
   "source": [
    "## 2. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee51e59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "test_size = 0.2\n",
    "random_state = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "print(f\"Test size: {test_size*100}%\")\n",
    "\n",
    "print(f\"\\nTraining set target distribution:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "print(f\"\\nTesting set target distribution:\")\n",
    "print(y_test.value_counts().sort_index())\n",
    "\n",
    "# Check if stratification worked\n",
    "train_props = y_train.value_counts(normalize=True).sort_index()\n",
    "test_props = y_test.value_counts(normalize=True).sort_index()\n",
    "\n",
    "print(f\"\\nProportions in training set: {train_props.values}\")\n",
    "print(f\"Proportions in testing set: {test_props.values}\")\n",
    "print(f\"Stratification successful: {np.allclose(train_props.values, test_props.values, atol=0.05)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e141e0",
   "metadata": {},
   "source": [
    "## 3. Initialize Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfedc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model trainer\n",
    "trainer = ModelTrainer()\n",
    "\n",
    "# Initialize all models with default parameters\n",
    "models = trainer.initialize_models()\n",
    "\n",
    "print(f\"Initialized {len(models)} models:\")\n",
    "for model_name, model in models.items():\n",
    "    print(f\"  - {model_name}: {type(model).__name__}\")\n",
    "\n",
    "print(f\"\\nTraining data shape: {X_train.shape}\")\n",
    "print(f\"Training target shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073ddf72",
   "metadata": {},
   "source": [
    "## 4. Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec6be89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "print(\"Training all models...\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "trained_models = trainer.train_all_models(X_train, y_train)\n",
    "\n",
    "print(f\"\\nAll {len(trained_models)} models trained successfully!\")\n",
    "print(f\"Trained models: {list(trained_models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60a42e6",
   "metadata": {},
   "source": [
    "## 5. Evaluate All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a74708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all trained models\n",
    "print(\"Evaluating all models...\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "model_scores = trainer.evaluate_all_models(X_test, y_test)\n",
    "\n",
    "print(f\"\\nModel evaluation completed!\")\n",
    "print(f\"Evaluated {len(model_scores)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed468c5",
   "metadata": {},
   "source": [
    "## 6. Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7fcefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive comparison of model performances\n",
    "scores_df = pd.DataFrame(model_scores).T\n",
    "scores_df = scores_df.round(4)\n",
    "\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "print(scores_df)\n",
    "\n",
    "# Visualize model comparison\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "n_metrics = len(metrics)\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "for i, metric in enumerate(metrics, 1):\n",
    "    plt.subplot(1, n_metrics, i)\n",
    "    metric_scores = scores_df[metric].sort_values(ascending=False)\n",
    "    bars = plt.bar(range(len(metric_scores)), metric_scores.values)\n",
    "    plt.title(f'{metric.title()} Comparison')\n",
    "    plt.ylabel(metric.title())\n",
    "    plt.xticks(range(len(metric_scores)), metric_scores.index, rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for j, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best model for each metric\n",
    "print(\"\\nBest models for each metric:\")\n",
    "for metric in metrics:\n",
    "    best_model = scores_df[metric].idxmax()\n",
    "    best_score = scores_df[metric].max()\n",
    "    print(f\"  {metric:12s}: {best_model:20s} ({best_score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4210028c",
   "metadata": {},
   "source": [
    "## 7. Cross-Validation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade2d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation for all models\n",
    "print(\"Performing cross-validation analysis...\")\n",
    "\n",
    "cv_results = {}\n",
    "for model_name in models.keys():\n",
    "    cv_result = trainer.cross_validate_model(model_name, X_train, y_train, cv=5)\n",
    "    cv_results[model_name] = cv_result\n",
    "\n",
    "# Create CV results DataFrame\n",
    "cv_df = pd.DataFrame({\n",
    "    model: {\n",
    "        'mean_cv_score': results['mean_cv_score'],\n",
    "        'std_cv_score': results['std_cv_score']\n",
    "    } for model, results in cv_results.items()\n",
    "}).T\n",
    "\n",
    "cv_df = cv_df.round(4)\n",
    "cv_df = cv_df.sort_values('mean_cv_score', ascending=False)\n",
    "\n",
    "print(\"\\nCROSS-VALIDATION RESULTS\")\n",
    "print(\"=\" * 40)\n",
    "print(cv_df)\n",
    "\n",
    "# Visualize CV results\n",
    "plt.figure(figsize=(12, 6))\n",
    "x_pos = np.arange(len(cv_df))\n",
    "\n",
    "plt.bar(x_pos, cv_df['mean_cv_score'], \n",
    "        yerr=cv_df['std_cv_score'], capsize=5, alpha=0.7)\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Cross-Validation Accuracy')\n",
    "plt.title('5-Fold Cross-Validation Results')\n",
    "plt.xticks(x_pos, cv_df.index, rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (mean_score, std_score) in enumerate(zip(cv_df['mean_cv_score'], cv_df['std_cv_score'])):\n",
    "    plt.text(i, mean_score + std_score + 0.01, f'{mean_score:.3f}', \n",
    "             ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f24e06",
   "metadata": {},
   "source": [
    "## 8. Get Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff6ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best performing model\n",
    "best_model_name, best_model, best_scores = trainer.get_best_model()\n",
    "\n",
    "print(f\"BEST MODEL SELECTION\")\n",
    "print(f\"=\" * 30)\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Model Type: {type(best_model).__name__}\")\n",
    "print(f\"\\nBest Model Performance:\")\n",
    "for metric, score in best_scores.items():\n",
    "    print(f\"  {metric:12s}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nBest Model CV Score: {cv_results[best_model_name]['mean_cv_score']:.4f} \"\n",
    "      f\"(±{cv_results[best_model_name]['std_cv_score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2577d184",
   "metadata": {},
   "source": [
    "## 9. Detailed Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1e8354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed classification report for the best model\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Classification report\n",
    "class_names = ['Low Stress', 'Medium Stress', 'High Stress']  # Adjust based on your target encoding\n",
    "report = classification_report(y_test, y_pred_best, target_names=class_names)\n",
    "\n",
    "print(f\"DETAILED CLASSIFICATION REPORT - {best_model_name.upper()}\")\n",
    "print(\"=\" * 60)\n",
    "print(report)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title(f'Confusion Matrix - {best_model_name.title()}')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "print(f\"\\nPer-class Accuracy:\")\n",
    "for i, (class_name, acc) in enumerate(zip(class_names, class_accuracy)):\n",
    "    print(f\"  {class_name:15s}: {acc:.4f} ({cm[i,i]}/{cm[i,:].sum()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e780d7f",
   "metadata": {},
   "source": [
    "## 10. Hyperparameter Tuning for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d18508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hyperparameter tuning for the best model\n",
    "print(f\"Performing hyperparameter tuning for {best_model_name}...\")\n",
    "\n",
    "# Define parameter grids for different models\n",
    "param_grids = {\n",
    "    'random_forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'C': [0.1, 1.0, 10.0],\n",
    "        'solver': ['liblinear', 'lbfgs']\n",
    "    },\n",
    "    'svm': {\n",
    "        'C': [0.1, 1.0, 10.0],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    }\n",
    "}\n",
    "\n",
    "if best_model_name in param_grids:\n",
    "    param_grid = param_grids[best_model_name]\n",
    "    \n",
    "    print(f\"Tuning hyperparameters: {list(param_grid.keys())}\")\n",
    "    \n",
    "    # Perform hyperparameter tuning\n",
    "    tuned_model = trainer.hyperparameter_tuning(\n",
    "        best_model_name, param_grid, X_train, y_train, cv=3\n",
    "    )\n",
    "    \n",
    "    # Re-train and evaluate the tuned model\n",
    "    trainer.train_model(best_model_name, X_train, y_train)\n",
    "    tuned_scores = trainer.evaluate_model(best_model_name, X_test, y_test)\n",
    "    \n",
    "    print(f\"\\nPerformance comparison:\")\n",
    "    print(f\"Original {best_model_name} accuracy: {best_scores['accuracy']:.4f}\")\n",
    "    print(f\"Tuned {best_model_name} accuracy: {tuned_scores['accuracy']:.4f}\")\n",
    "    print(f\"Improvement: {tuned_scores['accuracy'] - best_scores['accuracy']:.4f}\")\n",
    "    \n",
    "    # Update best model if tuning improved performance\n",
    "    if tuned_scores['accuracy'] > best_scores['accuracy']:\n",
    "        best_model = trainer.trained_models[best_model_name]\n",
    "        best_scores = tuned_scores\n",
    "        print(f\"✓ Model performance improved with hyperparameter tuning!\")\n",
    "    else:\n",
    "        print(f\"• Original model performs better or equal.\")\n",
    "else:\n",
    "    print(f\"No parameter grid defined for {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3b8354",
   "metadata": {},
   "source": [
    "## 11. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf50744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance for the best model (if available)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # Get feature importances\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"FEATURE IMPORTANCE - {best_model_name.upper()}\")\n",
    "    print(\"=\" * 40)\n",
    "    print(feature_importance)\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance['feature'], feature_importance['importance'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Feature Importance - {best_model_name.title()}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    # For linear models, show coefficients\n",
    "    coefficients = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'coefficient': best_model.coef_[0] if best_model.coef_.ndim > 1 else best_model.coef_\n",
    "    })\n",
    "    coefficients['abs_coefficient'] = np.abs(coefficients['coefficient'])\n",
    "    coefficients = coefficients.sort_values('abs_coefficient', ascending=False)\n",
    "    \n",
    "    print(f\"FEATURE COEFFICIENTS - {best_model_name.upper()}\")\n",
    "    print(\"=\" * 40)\n",
    "    print(coefficients)\n",
    "    \n",
    "    # Visualize coefficients\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = ['red' if x < 0 else 'blue' for x in coefficients['coefficient']]\n",
    "    plt.barh(coefficients['feature'], coefficients['coefficient'], color=colors, alpha=0.7)\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.title(f'Feature Coefficients - {best_model_name.title()}')\n",
    "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(f\"Feature importance not available for {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15be5284",
   "metadata": {},
   "source": [
    "## 12. Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a8b0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all trained models\n",
    "print(\"Saving trained models...\")\n",
    "\n",
    "model_save_paths = {}\n",
    "for model_name in trained_models.keys():\n",
    "    model_path = MODELS_DIR / f\"{model_name}_model.joblib\"\n",
    "    trainer.save_model(model_name, str(model_path))\n",
    "    model_save_paths[model_name] = str(model_path)\n",
    "\n",
    "print(f\"\\nSaved {len(model_save_paths)} models to {MODELS_DIR}\")\n",
    "\n",
    "# Save model comparison results\n",
    "import json\n",
    "\n",
    "model_results = {\n",
    "    'best_model': {\n",
    "        'name': best_model_name,\n",
    "        'scores': best_scores,\n",
    "        'cv_score': cv_results[best_model_name]['mean_cv_score'],\n",
    "        'cv_std': cv_results[best_model_name]['std_cv_score']\n",
    "    },\n",
    "    'all_model_scores': model_scores,\n",
    "    'cv_results': {k: v for k, v in cv_results.items()},\n",
    "    'model_ranking': scores_df.sort_values('accuracy', ascending=False).index.tolist(),\n",
    "    'dataset_info': {\n",
    "        'n_features': X.shape[1],\n",
    "        'n_samples': len(df),\n",
    "        'n_train': len(X_train),\n",
    "        'n_test': len(X_test),\n",
    "        'feature_names': list(X.columns)\n",
    "    },\n",
    "    'model_save_paths': model_save_paths\n",
    "}\n",
    "\n",
    "results_path = RESULTS_DIR / \"model_training_results.json\"\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(model_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Model training results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f386c5",
   "metadata": {},
   "source": [
    "## 13. Model Development Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca806a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive model development summary\n",
    "print(\"MODEL DEVELOPMENT SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dataset: {len(df)} samples, {X.shape[1]} features\")\n",
    "print(f\"Train/Test Split: {len(X_train)}/{len(X_test)} ({(1-test_size)*100:.0f}%/{test_size*100:.0f}%)\")\n",
    "print(f\"\\nModels Trained: {len(trained_models)}\")\n",
    "for model_name in trained_models.keys():\n",
    "    print(f\"  ✓ {model_name}\")\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"Best Accuracy: {best_scores['accuracy']:.4f}\")\n",
    "print(f\"Best F1-Score: {best_scores['f1_score']:.4f}\")\n",
    "print(f\"Cross-Validation: {cv_results[best_model_name]['mean_cv_score']:.4f} (±{cv_results[best_model_name]['std_cv_score']:.4f})\")\n",
    "\n",
    "print(f\"\\nModel Rankings (by accuracy):\")\n",
    "for i, model_name in enumerate(scores_df.sort_values('accuracy', ascending=False).index, 1):\n",
    "    accuracy = scores_df.loc[model_name, 'accuracy']\n",
    "    print(f\"  {i}. {model_name:20s}: {accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\nAll models saved to: {MODELS_DIR}\")\n",
    "print(f\"Results saved to: {results_path}\")\n",
    "\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"1. Model evaluation and visualization\")\n",
    "print(f\"2. Generate prediction reports\")\n",
    "print(f\"3. Deploy the best model for real-time predictions\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
