{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c27b527d",
   "metadata": {},
   "source": [
    "# Data Cleaning - Stress Level Prediction\n",
    "\n",
    "This notebook performs data cleaning and preprocessing for the stress level prediction project.\n",
    "\n",
    "## Objectives:\n",
    "1. Handle missing values\n",
    "2. Remove duplicates\n",
    "3. Handle outliers\n",
    "4. Encode categorical variables\n",
    "5. Feature scaling and normalization\n",
    "6. Data validation and quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624e5342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from utils.config import *\n",
    "from data.data_loader import DataLoader\n",
    "from data.data_preprocessor import DataPreprocessor\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff2fca8",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94b6045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "data_loader = DataLoader()\n",
    "\n",
    "# Load your dataset - Replace with your actual data loading\n",
    "# df = data_loader.load_csv('your_dataset.csv')\n",
    "\n",
    "# For demonstration, we'll recreate the sample dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "sample_data = {\n",
    "    'heart_rate': np.random.normal(75, 15, n_samples),\n",
    "    'sleep_hours': np.random.normal(7, 1.5, n_samples),\n",
    "    'exercise_minutes': np.random.exponential(30, n_samples),\n",
    "    'caffeine_intake': np.random.poisson(2, n_samples),\n",
    "    'work_hours': np.random.normal(8, 2, n_samples),\n",
    "    'age': np.random.randint(18, 65, n_samples),\n",
    "    'bmi': np.random.normal(25, 4, n_samples),\n",
    "    'blood_pressure_sys': np.random.normal(120, 20, n_samples),\n",
    "    'blood_pressure_dia': np.random.normal(80, 10, n_samples),\n",
    "    'gender': np.random.choice(['Male', 'Female'], n_samples),\n",
    "    'stress_level': np.random.choice(['Low', 'Medium', 'High'], n_samples, p=[0.3, 0.5, 0.2])\n",
    "}\n",
    "\n",
    "df_original = pd.DataFrame(sample_data)\n",
    "\n",
    "# Introduce some data quality issues for demonstration\n",
    "# Missing values\n",
    "missing_indices = np.random.choice(df_original.index, size=int(0.05 * len(df_original)), replace=False)\n",
    "df_original.loc[missing_indices, 'sleep_hours'] = np.nan\n",
    "\n",
    "# Add some duplicates\n",
    "duplicates = df_original.sample(n=10, random_state=42)\n",
    "df_original = pd.concat([df_original, duplicates], ignore_index=True)\n",
    "\n",
    "# Add some outliers\n",
    "outlier_indices = np.random.choice(df_original.index, size=20, replace=False)\n",
    "df_original.loc[outlier_indices, 'heart_rate'] = np.random.normal(150, 10, 20)  # Extreme heart rates\n",
    "\n",
    "df = df_original.copy()\n",
    "\n",
    "print(f\"Original dataset loaded!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1045fddf",
   "metadata": {},
   "source": [
    "## 2. Initial Data Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbc0742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get initial data assessment\n",
    "print(\"Initial Data Assessment:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Data types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nDuplicate rows: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335c12d2",
   "metadata": {},
   "source": [
    "## 3. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1670a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Check missing values before handling\n",
    "print(\"Missing values before handling:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Handle missing values\n",
    "df_cleaned = preprocessor.handle_missing_values(df, strategy='mean')\n",
    "\n",
    "print(\"\\nMissing values after handling:\")\n",
    "print(df_cleaned.isnull().sum())\n",
    "\n",
    "# Visualize the impact\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    df.isnull().sum().plot(kind='bar')\n",
    "    plt.title('Missing Values Before')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    df_cleaned.isnull().sum().plot(kind='bar')\n",
    "    plt.title('Missing Values After')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "df = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ca15c9",
   "metadata": {},
   "source": [
    "## 4. Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59274e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "print(f\"Rows before removing duplicates: {len(df)}\")\n",
    "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "df_no_duplicates = preprocessor.remove_duplicates(df)\n",
    "\n",
    "print(f\"Rows after removing duplicates: {len(df_no_duplicates)}\")\n",
    "print(f\"Rows removed: {len(df) - len(df_no_duplicates)}\")\n",
    "\n",
    "df = df_no_duplicates.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db7955b",
   "metadata": {},
   "source": [
    "## 5. Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9815f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect and visualize outliers\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "def detect_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "print(\"Outlier Analysis:\")\n",
    "outlier_summary = {}\n",
    "for col in numerical_cols:\n",
    "    outliers, lower, upper = detect_outliers_iqr(df, col)\n",
    "    outlier_count = len(outliers)\n",
    "    outlier_percentage = (outlier_count / len(df)) * 100\n",
    "    outlier_summary[col] = {\n",
    "        'count': outlier_count,\n",
    "        'percentage': outlier_percentage,\n",
    "        'lower_bound': lower,\n",
    "        'upper_bound': upper\n",
    "    }\n",
    "    print(f\"{col}: {outlier_count} outliers ({outlier_percentage:.2f}%)\")\n",
    "\n",
    "# Visualize outliers\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(numerical_cols, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    plt.boxplot(df[col])\n",
    "    plt.title(f'{col} - Outliers')\n",
    "    plt.ylabel(col)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d9e1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3677b324",
   "metadata": {},
   "source": [
    "## 6. Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cf023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(exclude=[np.number]).columns\n",
    "target_col = 'stress_level'\n",
    "\n",
    "print(f\"Categorical columns: {list(categorical_cols)}\")\n",
    "print(f\"Target column: {target_col}\")\n",
    "\n",
    "# Show unique values for each categorical column\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col} unique values: {df[col].unique()}\")\n",
    "    print(f\"Value counts:\")\n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a6f4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "print(\"Encoding categorical variables...\")\n",
    "print(f\"Shape before encoding: {df.shape}\")\n",
    "\n",
    "df_encoded = preprocessor.encode_categorical_variables(df, target_col=target_col)\n",
    "\n",
    "print(f\"Shape after encoding: {df_encoded.shape}\")\n",
    "print(f\"New columns: {set(df_encoded.columns) - set(df.columns)}\")\n",
    "\n",
    "# Show the target column transformation\n",
    "if target_col in preprocessor.encoders:\n",
    "    le = preprocessor.encoders[target_col]\n",
    "    print(f\"\\nTarget encoding:\")\n",
    "    for i, class_label in enumerate(le.classes_):\n",
    "        print(f\"  {class_label} -> {i}\")\n",
    "\n",
    "df = df_encoded.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc02baaa",
   "metadata": {},
   "source": [
    "## 7. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac12745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(y.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ec0bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical features\n",
    "print(\"Before scaling - Feature statistics:\")\n",
    "print(X.describe())\n",
    "\n",
    "X_scaled, _ = preprocessor.scale_features(X)\n",
    "\n",
    "print(\"\\nAfter scaling - Feature statistics:\")\n",
    "print(X_scaled.describe())\n",
    "\n",
    "# Visualize the effect of scaling\n",
    "numerical_features = X.select_dtypes(include=[np.number]).columns[:6]  # Show first 6 for space\n",
    "\n",
    "if len(numerical_features) > 0:\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    for i, col in enumerate(numerical_features, 1):\n",
    "        plt.subplot(2, 3, i)\n",
    "        plt.hist(X[col], alpha=0.7, label='Original', bins=30)\n",
    "        plt.hist(X_scaled[col], alpha=0.7, label='Scaled', bins=30)\n",
    "        plt.title(f'{col}')\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "X = X_scaled.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a507259",
   "metadata": {},
   "source": [
    "## 8. Remove Low Variance Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da525168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove low variance features\n",
    "print(f\"Features before variance filtering: {X.shape[1]}\")\n",
    "\n",
    "# Combine features and target for variance analysis\n",
    "df_for_variance = pd.concat([X, y], axis=1)\n",
    "df_variance_filtered = preprocessor.remove_low_variance_features(df_for_variance, threshold=0.01)\n",
    "\n",
    "# Separate again\n",
    "X_variance_filtered = df_variance_filtered.drop(columns=[target_col])\n",
    "y_variance_filtered = df_variance_filtered[target_col]\n",
    "\n",
    "print(f\"Features after variance filtering: {X_variance_filtered.shape[1]}\")\n",
    "removed_features = set(X.columns) - set(X_variance_filtered.columns)\n",
    "if removed_features:\n",
    "    print(f\"Removed low variance features: {removed_features}\")\n",
    "else:\n",
    "    print(\"No low variance features found.\")\n",
    "\n",
    "X = X_variance_filtered.copy()\n",
    "y = y_variance_filtered.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e027d3da",
   "metadata": {},
   "source": [
    "## 9. Final Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e701d1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final data quality assessment\n",
    "final_df = pd.concat([X, y], axis=1)\n",
    "\n",
    "print(\"Final Data Quality Report:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Final dataset shape: {final_df.shape}\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Samples: {len(final_df)}\")\n",
    "print(f\"Missing values: {final_df.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate rows: {final_df.duplicated().sum()}\")\n",
    "print(f\"Data types:\")\n",
    "for dtype in final_df.dtypes.value_counts().items():\n",
    "    print(f\"  {dtype[0]}: {dtype[1]} columns\")\n",
    "\n",
    "print(f\"\\nTarget variable distribution:\")\n",
    "print(y.value_counts().sort_index())\n",
    "print(f\"\\nTarget variable proportions:\")\n",
    "print(y.value_counts(normalize=True).sort_index())\n",
    "\n",
    "# Memory usage\n",
    "memory_usage = final_df.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "print(f\"\\nMemory usage: {memory_usage:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c193ba",
   "metadata": {},
   "source": [
    "## 10. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ec5387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset\n",
    "processed_data_path = PROCESSED_DATA_DIR / \"cleaned_stress_data.csv\"\n",
    "final_df.to_csv(processed_data_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to: {processed_data_path}\")\n",
    "\n",
    "# Save feature and target separately for convenience\n",
    "features_path = PROCESSED_DATA_DIR / \"features.csv\"\n",
    "target_path = PROCESSED_DATA_DIR / \"target.csv\"\n",
    "\n",
    "X.to_csv(features_path, index=False)\n",
    "y.to_csv(target_path, index=False)\n",
    "\n",
    "print(f\"Features saved to: {features_path}\")\n",
    "print(f\"Target saved to: {target_path}\")\n",
    "\n",
    "# Save preprocessing information\n",
    "preprocessing_info = {\n",
    "    'original_shape': df_original.shape,\n",
    "    'final_shape': final_df.shape,\n",
    "    'missing_values_handled': df_original.isnull().sum().sum(),\n",
    "    'duplicates_removed': df_original.duplicated().sum(),\n",
    "    'features_removed': len(df_original.columns) - len(final_df.columns),\n",
    "    'target_column': target_col,\n",
    "    'feature_columns': list(X.columns),\n",
    "    'categorical_encodings': {k: list(v.classes_) for k, v in preprocessor.encoders.items()}\n",
    "}\n",
    "\n",
    "import json\n",
    "info_path = PROCESSED_DATA_DIR / \"preprocessing_info.json\"\n",
    "with open(info_path, 'w') as f:\n",
    "    json.dump(preprocessing_info, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Preprocessing information saved to: {info_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35493f58",
   "metadata": {},
   "source": [
    "## 11. Data Cleaning Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23249a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive cleaning summary\n",
    "print(\"DATA CLEANING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original dataset shape: {df_original.shape}\")\n",
    "print(f\"Final dataset shape: {final_df.shape}\")\n",
    "print(f\"Rows removed: {df_original.shape[0] - final_df.shape[0]}\")\n",
    "print(f\"Columns changed: {df_original.shape[1]} -> {final_df.shape[1]}\")\n",
    "\n",
    "print(\"\\nData Quality Improvements:\")\n",
    "print(f\"✓ Missing values: {df_original.isnull().sum().sum()} -> {final_df.isnull().sum().sum()}\")\n",
    "print(f\"✓ Duplicate rows: {df_original.duplicated().sum()} -> {final_df.duplicated().sum()}\")\n",
    "print(f\"✓ Categorical variables encoded\")\n",
    "print(f\"✓ Numerical features scaled\")\n",
    "print(f\"✓ Low variance features removed\")\n",
    "print(f\"✓ Outliers handled\")\n",
    "\n",
    "print(\"\\nDataset is now ready for feature selection and model training!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Feature selection and engineering\")\n",
    "print(\"2. Train-test split\")\n",
    "print(\"3. Model training and evaluation\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
