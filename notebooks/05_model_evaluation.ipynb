{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18b7dc64",
   "metadata": {},
   "source": [
    "# Model Evaluation - Stress Level Prediction\n",
    "\n",
    "This notebook provides comprehensive evaluation and visualization of the trained models.\n",
    "\n",
    "## Objectives:\n",
    "1. Load trained models and test data\n",
    "2. Generate detailed performance metrics\n",
    "3. Create comprehensive visualizations\n",
    "4. Analyze model strengths and weaknesses\n",
    "5. Generate evaluation report\n",
    "6. Make predictions on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88331a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from utils.config import *\n",
    "from models.model_evaluator import ModelEvaluator\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dfe326",
   "metadata": {},
   "source": [
    "## 1. Load Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf92704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the final dataset\n",
    "try:\n",
    "    final_dataset_path = PROCESSED_DATA_DIR / \"final_dataset.csv\"\n",
    "    df = pd.read_csv(final_dataset_path)\n",
    "    print(f\"Loaded dataset from: {final_dataset_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Dataset not found. Creating sample data for demonstration.\")\n",
    "    # Create sample data\n",
    "    np.random.seed(42)\n",
    "    n_samples = 800\n",
    "    sample_data = {\n",
    "        'heart_rate': np.random.normal(0, 1, n_samples),\n",
    "        'work_hours': np.random.normal(0, 1, n_samples),\n",
    "        'sleep_hours': np.random.normal(0, 1, n_samples),\n",
    "        'exercise_minutes': np.random.normal(0, 1, n_samples),\n",
    "        'bmi': np.random.normal(0, 1, n_samples),\n",
    "        'caffeine_intake': np.random.normal(0, 1, n_samples),\n",
    "        'stress_level': np.random.choice([0, 1, 2], n_samples, p=[0.3, 0.5, 0.2])\n",
    "    }\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    # Make features predictive\n",
    "    df.loc[df['stress_level'] == 2, 'heart_rate'] += 1.5\n",
    "    df.loc[df['stress_level'] == 2, 'work_hours'] += 1.2\n",
    "    df.loc[df['stress_level'] == 0, 'sleep_hours'] += 1.0\n",
    "\n",
    "# Separate features and target\n",
    "target_col = 'stress_level'\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "# Recreate train-test split (same as in model development)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Features: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625c5cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model training results\n",
    "try:\n",
    "    results_path = RESULTS_DIR / \"model_training_results.json\"\n",
    "    with open(results_path, 'r') as f:\n",
    "        model_results = json.load(f)\n",
    "    print(f\"Loaded model results from: {results_path}\")\n",
    "    \n",
    "    best_model_name = model_results['best_model']['name']\n",
    "    all_model_scores = model_results['all_model_scores']\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Model results not found. Using mock results for demonstration.\")\n",
    "    # Create mock results\n",
    "    best_model_name = 'random_forest'\n",
    "    all_model_scores = {\n",
    "        'random_forest': {'accuracy': 0.85, 'precision': 0.84, 'recall': 0.85, 'f1_score': 0.84},\n",
    "        'gradient_boosting': {'accuracy': 0.82, 'precision': 0.81, 'recall': 0.82, 'f1_score': 0.81},\n",
    "        'decision_tree': {'accuracy': 0.78, 'precision': 0.77, 'recall': 0.78, 'f1_score': 0.77},\n",
    "        'logistic_regression': {'accuracy': 0.75, 'precision': 0.74, 'recall': 0.75, 'f1_score': 0.74},\n",
    "        'svm': {'accuracy': 0.73, 'precision': 0.72, 'recall': 0.73, 'f1_score': 0.72}\n",
    "    }\n",
    "\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"Models evaluated: {list(all_model_scores.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c75072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained models\n",
    "trained_models = {}\n",
    "model_predictions = {}\n",
    "model_probabilities = {}\n",
    "\n",
    "# Try to load saved models, or create mock models for demonstration\n",
    "for model_name in all_model_scores.keys():\n",
    "    model_path = MODELS_DIR / f\"{model_name}_model.joblib\"\n",
    "    try:\n",
    "        model = joblib.load(model_path)\n",
    "        trained_models[model_name] = model\n",
    "        model_predictions[model_name] = model.predict(X_test)\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            model_probabilities[model_name] = model.predict_proba(X_test)\n",
    "        print(f\"Loaded {model_name} from {model_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Model file not found for {model_name}. Creating mock predictions.\")\n",
    "        # Create mock predictions for demonstration\n",
    "        np.random.seed(42)\n",
    "        accuracy = all_model_scores[model_name]['accuracy']\n",
    "        n_correct = int(len(y_test) * accuracy)\n",
    "        \n",
    "        # Create predictions with specified accuracy\n",
    "        predictions = y_test.copy().values\n",
    "        incorrect_indices = np.random.choice(len(predictions), \n",
    "                                           size=len(predictions)-n_correct, \n",
    "                                           replace=False)\n",
    "        for idx in incorrect_indices:\n",
    "            # Randomly change to different class\n",
    "            current = predictions[idx]\n",
    "            possible = [0, 1, 2]\n",
    "            possible.remove(current)\n",
    "            predictions[idx] = np.random.choice(possible)\n",
    "        \n",
    "        model_predictions[model_name] = predictions\n",
    "        \n",
    "        # Create mock probabilities\n",
    "        n_classes = len(np.unique(y))\n",
    "        probs = np.random.dirichlet(np.ones(n_classes), size=len(X_test))\n",
    "        model_probabilities[model_name] = probs\n",
    "\n",
    "print(f\"\\nLoaded/created predictions for {len(model_predictions)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ea922a",
   "metadata": {},
   "source": [
    "## 2. Initialize Model Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087433d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model evaluator\n",
    "evaluator = ModelEvaluator(figsize=(12, 8))\n",
    "\n",
    "# Define class names for better visualization\n",
    "class_names = ['Low Stress', 'Medium Stress', 'High Stress']\n",
    "\n",
    "print(\"Model evaluator initialized!\")\n",
    "print(f\"Class names: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de1436d",
   "metadata": {},
   "source": [
    "## 3. Detailed Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3472ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed classification reports for all models\n",
    "classification_reports = {}\n",
    "\n",
    "print(\"DETAILED CLASSIFICATION REPORTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_name, predictions in model_predictions.items():\n",
    "    print(f\"\\n{model_name.upper().replace('_', ' ')}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    report = evaluator.generate_classification_report(\n",
    "        y_test, predictions, target_names=class_names\n",
    "    )\n",
    "    classification_reports[model_name] = report\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fcc7fe",
   "metadata": {},
   "source": [
    "## 4. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0c110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrices for all models\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (model_name, predictions) in enumerate(model_predictions.items()):\n",
    "    if i < len(axes):\n",
    "        plt.sca(axes[i])\n",
    "        cm = confusion_matrix(y_test, predictions)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title(f'Confusion Matrix - {model_name.replace(\"_\", \" \").title()}')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "\n",
    "# Hide the last subplot if we have fewer than 6 models\n",
    "if len(model_predictions) < len(axes):\n",
    "    axes[-1].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save individual confusion matrices\n",
    "for model_name, predictions in model_predictions.items():\n",
    "    save_path = get_figure_save_path(f'confusion_matrix_{model_name}')\n",
    "    evaluator.plot_confusion_matrix(\n",
    "        y_test, predictions, labels=class_names, save_path=str(save_path)\n",
    "    )\n",
    "    plt.close()  # Close to save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20c4422",
   "metadata": {},
   "source": [
    "## 5. Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42650441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive model comparison\n",
    "comparison_fig = evaluator.compare_models_performance(\n",
    "    all_model_scores, \n",
    "    save_path=str(get_figure_save_path('model_performance_comparison'))\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Create a detailed comparison table\n",
    "scores_df = pd.DataFrame(all_model_scores).T\n",
    "scores_df = scores_df.round(4)\n",
    "scores_df['rank'] = scores_df['accuracy'].rank(ascending=False, method='min').astype(int)\n",
    "scores_df = scores_df.sort_values('rank')\n",
    "\n",
    "print(\"MODEL PERFORMANCE RANKING\")\n",
    "print(\"=\" * 50)\n",
    "print(scores_df[['rank', 'accuracy', 'precision', 'recall', 'f1_score']])\n",
    "\n",
    "# Highlight best performances\n",
    "print(\"\\nBest Performance per Metric:\")\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1_score']:\n",
    "    best_model = scores_df[metric].idxmax()\n",
    "    best_score = scores_df[metric].max()\n",
    "    print(f\"  {metric.title():12s}: {best_model:20s} ({best_score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb406460",
   "metadata": {},
   "source": [
    "## 6. ROC Curves (for Binary Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e51a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multiclass problems, we can create ROC curves for each class vs rest\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from itertools import cycle\n",
    "\n",
    "# Binarize the output for ROC analysis\n",
    "y_test_bin = label_binarize(y_test, classes=[0, 1, 2])\n",
    "n_classes = y_test_bin.shape[1]\n",
    "\n",
    "# Plot ROC curves for the best model\n",
    "if best_model_name in model_probabilities:\n",
    "    y_proba_best = model_probabilities[best_model_name]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    \n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_proba_best[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, color=color, lw=2,\n",
    "                label=f'{class_names[i]} (AUC = {roc_auc:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curves - {best_model_name.replace(\"_\", \" \").title()} (One-vs-Rest)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Save the plot\n",
    "    save_path = get_figure_save_path(f'roc_curves_{best_model_name}')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"ROC curves saved to: {save_path}\")\n",
    "else:\n",
    "    print(f\"Probability predictions not available for {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81c1109",
   "metadata": {},
   "source": [
    "## 7. Learning Curves (if models available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984a0917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate learning curves for the best model (if available)\n",
    "if best_model_name in trained_models:\n",
    "    best_model = trained_models[best_model_name]\n",
    "    \n",
    "    # Combine training and test data for learning curve\n",
    "    X_combined = pd.concat([X_train, X_test])\n",
    "    y_combined = pd.concat([y_train, y_test])\n",
    "    \n",
    "    learning_curve_fig = evaluator.plot_learning_curves(\n",
    "        best_model, X_combined, y_combined, \n",
    "        model_name=best_model_name.replace('_', ' ').title(),\n",
    "        save_path=str(get_figure_save_path(f'learning_curve_{best_model_name}'))\n",
    "    )\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Trained model not available for {best_model_name}. Skipping learning curves.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd11ed73",
   "metadata": {},
   "source": [
    "## 8. Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9f9848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze per-class performance for all models\n",
    "per_class_metrics = {}\n",
    "\n",
    "for model_name, predictions in model_predictions.items():\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    class_precision = cm.diagonal() / cm.sum(axis=0)\n",
    "    class_recall = cm.diagonal() / cm.sum(axis=1)\n",
    "    class_f1 = 2 * (class_precision * class_recall) / (class_precision + class_recall)\n",
    "    \n",
    "    per_class_metrics[model_name] = {\n",
    "        'precision': class_precision,\n",
    "        'recall': class_recall,\n",
    "        'f1_score': class_f1\n",
    "    }\n",
    "\n",
    "# Create per-class performance visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "metrics = ['precision', 'recall', 'f1_score']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    metric_data = []\n",
    "    for model_name in model_predictions.keys():\n",
    "        for class_idx, class_name in enumerate(class_names):\n",
    "            metric_data.append({\n",
    "                'Model': model_name.replace('_', ' ').title(),\n",
    "                'Class': class_name,\n",
    "                'Value': per_class_metrics[model_name][metric][class_idx]\n",
    "            })\n",
    "    \n",
    "    metric_df = pd.DataFrame(metric_data)\n",
    "    \n",
    "    # Create grouped bar plot\n",
    "    pivot_df = metric_df.pivot(index='Model', columns='Class', values='Value')\n",
    "    pivot_df.plot(kind='bar', ax=axes[i], rot=45)\n",
    "    axes[i].set_title(f'Per-Class {metric.title()}')\n",
    "    axes[i].set_ylabel(metric.title())\n",
    "    axes[i].legend(title='Stress Level')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_path = get_figure_save_path('per_class_performance')\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Per-class performance analysis saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f089ee8",
   "metadata": {},
   "source": [
    "## 9. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a63fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform error analysis for the best model\n",
    "best_predictions = model_predictions[best_model_name]\n",
    "\n",
    "# Find misclassified samples\n",
    "misclassified_mask = y_test != best_predictions\n",
    "misclassified_indices = y_test[misclassified_mask].index\n",
    "\n",
    "print(f\"ERROR ANALYSIS - {best_model_name.upper()}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total test samples: {len(y_test)}\")\n",
    "print(f\"Correctly classified: {(~misclassified_mask).sum()}\")\n",
    "print(f\"Misclassified: {misclassified_mask.sum()}\")\n",
    "print(f\"Error rate: {misclassified_mask.sum() / len(y_test):.4f}\")\n",
    "\n",
    "# Analyze error patterns\n",
    "error_patterns = pd.DataFrame({\n",
    "    'True_Label': y_test[misclassified_mask],\n",
    "    'Predicted_Label': best_predictions[misclassified_mask]\n",
    "})\n",
    "\n",
    "print(f\"\\nError Patterns:\")\n",
    "error_counts = error_patterns.groupby(['True_Label', 'Predicted_Label']).size().reset_index(name='Count')\n",
    "for _, row in error_counts.iterrows():\n",
    "    true_class = class_names[int(row['True_Label'])]\n",
    "    pred_class = class_names[int(row['Predicted_Label'])]\n",
    "    print(f\"  {true_class} -> {pred_class}: {row['Count']} cases\")\n",
    "\n",
    "# Visualize error patterns\n",
    "plt.figure(figsize=(8, 6))\n",
    "error_matrix = pd.crosstab(error_patterns['True_Label'], error_patterns['Predicted_Label'], \n",
    "                          rownames=['True'], colnames=['Predicted'])\n",
    "sns.heatmap(error_matrix, annot=True, fmt='d', cmap='Reds')\n",
    "plt.title(f'Error Pattern Matrix - {best_model_name.replace(\"_\", \" \").title()}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d34e86",
   "metadata": {},
   "source": [
    "## 10. Feature Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae38f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature impact on misclassifications\n",
    "if len(misclassified_indices) > 0:\n",
    "    # Compare feature distributions for correct vs incorrect predictions\n",
    "    X_test_reset = X_test.reset_index(drop=True)\n",
    "    y_test_reset = y_test.reset_index(drop=True)\n",
    "    \n",
    "    correct_mask = ~misclassified_mask\n",
    "    \n",
    "    print(f\"\\nFEATURE IMPACT ON MISCLASSIFICATIONS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Compare feature means\n",
    "    feature_comparison = pd.DataFrame({\n",
    "        'Correct_Mean': X_test_reset[correct_mask].mean(),\n",
    "        'Incorrect_Mean': X_test_reset[misclassified_mask].mean(),\n",
    "        'Difference': X_test_reset[misclassified_mask].mean() - X_test_reset[correct_mask].mean()\n",
    "    })\n",
    "    \n",
    "    feature_comparison['Abs_Difference'] = abs(feature_comparison['Difference'])\n",
    "    feature_comparison = feature_comparison.sort_values('Abs_Difference', ascending=False)\n",
    "    \n",
    "    print(\"Feature differences (Misclassified vs Correct):\")\n",
    "    print(feature_comparison.round(4))\n",
    "    \n",
    "    # Visualize feature distributions\n",
    "    n_features = min(4, len(X.columns))  # Show top 4 features\n",
    "    top_features = feature_comparison.head(n_features).index\n",
    "    \n",
    "    plt.figure(figsize=(16, 4))\n",
    "    for i, feature in enumerate(top_features, 1):\n",
    "        plt.subplot(1, n_features, i)\n",
    "        \n",
    "        plt.hist(X_test_reset[correct_mask][feature], alpha=0.7, \n",
    "                label='Correct', bins=20, density=True)\n",
    "        plt.hist(X_test_reset[misclassified_mask][feature], alpha=0.7, \n",
    "                label='Misclassified', bins=20, density=True)\n",
    "        \n",
    "        plt.title(f'{feature}')\n",
    "        plt.xlabel('Feature Value')\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = get_figure_save_path('feature_impact_analysis')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Feature impact analysis saved to: {save_path}\")\n",
    "else:\n",
    "    print(\"Perfect classification! No misclassified samples to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288e406c",
   "metadata": {},
   "source": [
    "## 11. Model Predictions on Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e3bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on a few sample cases\n",
    "sample_indices = np.random.choice(X_test.index, size=5, replace=False)\n",
    "sample_data = X_test.loc[sample_indices]\n",
    "sample_true = y_test.loc[sample_indices]\n",
    "\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    print(f\"\\nSample {idx}:\")\n",
    "    print(f\"True Label: {class_names[sample_true[idx]]}\")\n",
    "    \n",
    "    # Show predictions from all models\n",
    "    for model_name, predictions in model_predictions.items():\n",
    "        test_idx = list(X_test.index).index(idx)\n",
    "        pred_label = predictions[test_idx]\n",
    "        pred_class = class_names[pred_label]\n",
    "        \n",
    "        # Add confidence if available\n",
    "        confidence = \"\"\n",
    "        if model_name in model_probabilities:\n",
    "            proba = model_probabilities[model_name][test_idx]\n",
    "            max_proba = np.max(proba)\n",
    "            confidence = f\" (confidence: {max_proba:.3f})\"\n",
    "        \n",
    "        correct = \"‚úì\" if pred_label == sample_true[idx] else \"‚úó\"\n",
    "        print(f\"  {model_name:20s}: {pred_class:15s} {correct}{confidence}\")\n",
    "    \n",
    "    # Show feature values\n",
    "    print(f\"  Features: {sample_data.loc[idx].to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278c5039",
   "metadata": {},
   "source": [
    "## 12. Generate Evaluation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ef9d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive evaluation summary\n",
    "evaluation_summary = evaluator.generate_evaluation_summary(\n",
    "    all_model_scores, best_model_name\n",
    ")\n",
    "\n",
    "# Add additional analysis results\n",
    "evaluation_summary['error_analysis'] = {\n",
    "    'total_test_samples': len(y_test),\n",
    "    'misclassified_samples': misclassified_mask.sum(),\n",
    "    'error_rate': misclassified_mask.sum() / len(y_test),\n",
    "    'per_class_errors': error_patterns.groupby(['True_Label', 'Predicted_Label']).size().to_dict()\n",
    "}\n",
    "\n",
    "evaluation_summary['dataset_info'] = {\n",
    "    'n_features': X.shape[1],\n",
    "    'n_samples': len(df),\n",
    "    'n_test_samples': len(X_test),\n",
    "    'feature_names': list(X.columns),\n",
    "    'class_names': class_names,\n",
    "    'class_distribution': y_test.value_counts().to_dict()\n",
    "}\n",
    "\n",
    "print(\"\\nEVALUATION SUMMARY GENERATED\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Best Model: {evaluation_summary['best_model']}\")\n",
    "print(f\"Best Accuracy: {evaluation_summary['best_model_scores']['accuracy']:.4f}\")\n",
    "print(f\"Error Rate: {evaluation_summary['error_analysis']['error_rate']:.4f}\")\n",
    "print(f\"Models Evaluated: {len(evaluation_summary['all_model_scores'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f91c3d",
   "metadata": {},
   "source": [
    "## 13. Save Evaluation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfa0bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive evaluation report\n",
    "report_path = get_results_save_path('model_evaluation_report')\n",
    "evaluator.save_evaluation_report(str(report_path), evaluation_summary)\n",
    "\n",
    "# Save evaluation summary as JSON\n",
    "json_path = RESULTS_DIR / \"evaluation_summary.json\"\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(evaluation_summary, f, indent=2, default=str)\n",
    "\n",
    "# Save detailed results\n",
    "detailed_results = {\n",
    "    'classification_reports': classification_reports,\n",
    "    'per_class_metrics': {k: {metric: v[metric].tolist() for metric in v.keys()} \n",
    "                         for k, v in per_class_metrics.items()},\n",
    "    'feature_comparison': feature_comparison.to_dict() if len(misclassified_indices) > 0 else {},\n",
    "    'model_scores_df': scores_df.to_dict()\n",
    "}\n",
    "\n",
    "detailed_path = RESULTS_DIR / \"detailed_evaluation_results.json\"\n",
    "with open(detailed_path, 'w') as f:\n",
    "    json.dump(detailed_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nEvaluation reports saved:\")\n",
    "print(f\"  Text report: {report_path}\")\n",
    "print(f\"  Summary JSON: {json_path}\")\n",
    "print(f\"  Detailed JSON: {detailed_path}\")\n",
    "print(f\"  Figures saved to: {FIGURES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cac9ea",
   "metadata": {},
   "source": [
    "## 14. Final Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6936d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final recommendations\n",
    "print(\"FINAL RECOMMENDATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "best_accuracy = evaluation_summary['best_model_scores']['accuracy']\n",
    "best_f1 = evaluation_summary['best_model_scores']['f1_score']\n",
    "\n",
    "print(f\"‚úì Best Model: {best_model_name.replace('_', ' ').title()}\")\n",
    "print(f\"‚úì Model Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.1f}%)\")\n",
    "print(f\"‚úì Model F1-Score: {best_f1:.4f}\")\n",
    "\n",
    "if best_accuracy >= 0.85:\n",
    "    print(f\"\\nüéØ EXCELLENT: Model shows excellent performance for deployment\")\n",
    "elif best_accuracy >= 0.80:\n",
    "    print(f\"\\n‚úÖ GOOD: Model shows good performance, suitable for most applications\")\n",
    "elif best_accuracy >= 0.75:\n",
    "    print(f\"\\n‚ö†Ô∏è  FAIR: Model shows fair performance, consider improvements\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå POOR: Model needs significant improvement before deployment\")\n",
    "\n",
    "print(f\"\\nActionable Insights:\")\n",
    "\n",
    "# Performance insights\n",
    "if best_f1 < best_accuracy - 0.05:\n",
    "    print(f\"‚Ä¢ Consider class balancing techniques (F1 significantly lower than accuracy)\")\n",
    "\n",
    "# Error analysis insights\n",
    "error_rate = evaluation_summary['error_analysis']['error_rate']\n",
    "if error_rate > 0.2:\n",
    "    print(f\"‚Ä¢ High error rate ({error_rate:.1%}) - consider feature engineering or data collection\")\n",
    "\n",
    "# Model selection insights\n",
    "model_ranking = evaluation_summary['model_ranking']\n",
    "if len(model_ranking) > 1:\n",
    "    second_best = model_ranking[1]\n",
    "    second_best_acc = all_model_scores[second_best]['accuracy']\n",
    "    if abs(best_accuracy - second_best_acc) < 0.02:\n",
    "        print(f\"‚Ä¢ Consider ensemble methods - {second_best} performs similarly\")\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(f\"1. Deploy {best_model_name.replace('_', ' ').title()} for production use\")\n",
    "print(f\"2. Set up monitoring for model performance in production\")\n",
    "print(f\"3. Collect more data to improve model performance\")\n",
    "print(f\"4. Consider ensemble methods for improved accuracy\")\n",
    "print(f\"5. Implement model retraining pipeline for continuous improvement\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(f\"MODEL EVALUATION COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"All results and visualizations saved to: {REPORTS_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
